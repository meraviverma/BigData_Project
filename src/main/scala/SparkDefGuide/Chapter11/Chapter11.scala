package SparkDefGuide.Chapter11

import conf.SparkConfiguration
/*
Dataframe: Dataframe, which are datasets of type Row.
Dataset: Dataset are a strictly JVM language features that work with scala and java
Using dataset, we can define the object that each row in your dataset will consist of.
In scala we have case class object that defines the schema that we can use.
Scala has types like StringType, BigIntType,StructType
Encoder maps the domain-specific type T to spark's internal type system.


 */
object Chapter11 extends  SparkConfiguration{
  def main(args: Array[String]): Unit = {

  }


}
